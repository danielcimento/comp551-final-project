{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Import statements and global helper methods\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from tqdm import tqdm_notebook\n",
    "from gensim.models import word2vec\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c946df43a04d47aca5b474f256d265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74c8abcb607453e9a23924521fc74f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ff178158ce4c37965a4e94e6dc7b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb9d98e5a4641b6bae91e7ca46cd2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in the data sets\n",
    "\n",
    "raw_sentences = []\n",
    "\n",
    "with open(\"datasets/datasetSentences.txt\", encoding=\"utf-8\") as sentence_file:\n",
    "    # Skip header line\n",
    "    next(sentence_file)\n",
    "    for line in tqdm_notebook(sentence_file):\n",
    "        sentence = line.split(\"\\t\")[1].strip()\n",
    "        raw_sentences.append(sentence)\n",
    "    \n",
    "sentiment_map = {}\n",
    "with open(\"datasets/sentiment_labels.txt\") as sentiment_file:\n",
    "    next(sentiment_file)\n",
    "    for line in tqdm_notebook(sentiment_file):\n",
    "        index, score = tuple(line.split(\"|\"))\n",
    "        score = float(score.strip())\n",
    "        sentiment_map[index] = score\n",
    "    \n",
    "sentence_to_phrase_map = {}\n",
    "with open(\"datasets/dictionary.txt\", encoding=\"utf-8\") as dictionary_file:\n",
    "    for line in tqdm_notebook(dictionary_file):\n",
    "        phrase, phrase_num = tuple(line.split(\"|\"))\n",
    "        sentence_to_phrase_map[phrase] = phrase_num.strip()\n",
    "\n",
    "labelled_sentences = [(sentence, sentiment_map[sentence_to_phrase_map[sentence]]) for sentence in raw_sentences]\n",
    "            \n",
    "splits = ([], [], [])\n",
    "\n",
    "with open(\"datasets/datasetSplit.txt\") as split_file:\n",
    "    next(split_file)\n",
    "    for line in tqdm_notebook(split_file):\n",
    "        index, split = tuple(line.split(\",\"))\n",
    "        index, split = int(index), int(split)\n",
    "        # Splits are labelled 1-3\n",
    "        splits[split - 1].append(labelled_sentences[index - 1])\n",
    "        \n",
    "train_set, test_set, dev_set = splits\n",
    "train_sentences, train_y = zip(*train_set)\n",
    "dev_sentences, dev_y = zip(*dev_set)\n",
    "test_sentences, test_y = zip(*test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools for feature extraction\n",
    "\n",
    "# When lemmatizing, we need to convert from NLTK's part of speec\n",
    "# to wordnet's recognized parts of speech\n",
    "def get_wordnet_pos(treebank_pos):\n",
    "    if treebank_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def sentence_tokenize(sentence, lem = WordNetLemmatizer()):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return [lem.lemmatize(w, pos=get_wordnet_pos(pos)) for (w, pos) in tagged_tokens]\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    input = \"content\",\n",
    "    tokenizer = sentence_tokenize\n",
    ")\n",
    "\n",
    "tuple_count_vectorizer = CountVectorizer(\n",
    "    input = \"content\",\n",
    "    tokenizer = sentence_tokenize,\n",
    "    ngram_range = (2, 2)\n",
    ")\n",
    "\n",
    "# Fit all the sentences in the training set\n",
    "count_vectorizer.fit(train_sentences)\n",
    "tuple_count_vectorizer.fit(train_sentences)\n",
    "\n",
    "def count_vectorize(sentences, ngram=False):\n",
    "    if ngram:\n",
    "        return tuple_count_vectorizer.transform(sentences)\n",
    "    else:\n",
    "        return count_vectorizer.transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405ea2c5720b418da2bad24aeb6a4317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8544), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557f5df7f9344662b8a5a23e5717217d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9176acb29f48ccb7ab4e62276e6e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2210), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc391ff0e4dc43179713f99716196205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8544), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25087e97a20d43d7912badaeb625edff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedbdede77464bfd9205e8954dbcdad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2210), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Label conversion\n",
    "def coarse_label(sentiment):\n",
    "    if sentiment >= 0.5:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Negative\"\n",
    "    \n",
    "def fine_label(sentiment):\n",
    "    if sentiment < 0.2:\n",
    "        return \"Very Negative\"\n",
    "    elif sentiment < 0.4:\n",
    "        return \"Negative\"\n",
    "    elif sentiment < 0.6:\n",
    "        return \"Neutral\"\n",
    "    elif sentiment < 0.8:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Very Positive\"\n",
    "    \n",
    "coarse_train_y = [coarse_label(y) for y in tqdm_notebook(train_y)]\n",
    "coarse_dev_y = [coarse_label(y) for y in tqdm_notebook(dev_y)]\n",
    "coarse_test_y = [coarse_label(y) for y in tqdm_notebook(test_y)]\n",
    "\n",
    "fine_train_y = [fine_label(y) for y in tqdm_notebook(train_y)]\n",
    "fine_dev_y = [fine_label(y) for y in tqdm_notebook(dev_y)]\n",
    "fine_test_y = [fine_label(y) for y in tqdm_notebook(test_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Setup\n",
    "ps = PredefinedSplit([0 for s in test_sentences] + [1 for s in dev_sentences])\n",
    "nb_grid = {\"alpha\": [1e-4, 0.01, 0.1, 1.0, 2.0, 10.0]}\n",
    "\n",
    "svm_grid = {\n",
    "#         \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "#         \"degree\": range(1, 20, 2),\n",
    "        \"C\": [1e-4, 0.01, 0.1, 1.0, 2.0, 10.0], \n",
    "        \"tol\": [1e-4, 0.01, 0.1, 1.0, 2.0, 10.0],\n",
    "        \"max_iter\": range(1000, 10001, 1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing\n",
    "\n",
    "count_Xs = count_vectorize(train_sentences + dev_sentences)\n",
    "count_test_Xs = count_vectorize(test_sentences)\n",
    "\n",
    "bigram_Xs = count_vectorize(train_sentences + dev_sentences, ngram=True)\n",
    "bigram_test_Xs = count_vectorize(test_sentences, ngram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "# BernoulliNB Training (Coarse)\n",
    "bernoulli_naive_bayes = GridSearchCV(BernoulliNB(), nb_grid, cv=ps)\n",
    "bernoulli_naive_bayes.fit(count_Xs, coarse_train_y + coarse_dev_y)\n",
    "\n",
    "# BernoulliNB Results (Coarse)\n",
    "print(bernoulli_naive_bayes.score(count_test_Xs, coarse_test_y))\n",
    "print(bernoulli_naive_bayes.best_params_)\n",
    "\n",
    "# BernoulliNB Training (Fine)\n",
    "bernoulli_naive_bayes.fit(count_Xs, fine_train_y + fine_dev_y)\n",
    "\n",
    "# BernoulliNB Results (Fine)\n",
    "print(bernoulli_naive_bayes.score(count_test_Xs, fine_test_y))\n",
    "print(bernoulli_naive_bayes.best_params_)\n",
    "\n",
    "# MultinomialNB Training (Coarse)\n",
    "multinomial_naive_bayes = GridSearchCV(MultinomialNB(), nb_grid, cv=ps)\n",
    "multinomial_naive_bayes.fit(count_Xs, coarse_train_y + coarse_dev_y)\n",
    "\n",
    "# MultinomialNB Results (Coarse)\n",
    "print(multinomial_naive_bayes.score(count_test_Xs, coarse_test_y))\n",
    "print(multinomial_naive_bayes.best_params_)\n",
    "\n",
    "# MultinomialNB Training (Fine)\n",
    "multinomial_naive_bayes.fit(count_Xs, fine_train_y + fine_dev_y)\n",
    "\n",
    "# MultinomialNB Results (Fine)\n",
    "print(multinomial_naive_bayes.score(count_test_Xs, fine_test_y))\n",
    "print(multinomial_naive_bayes.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Naive Bayes\n",
    "\n",
    "# BernoulliNB Training (Coarse)\n",
    "bernoulli_bigram_nb = GridSearchCV(BernoulliNB(), nb_grid, cv=ps)\n",
    "bernoulli_bigram_nb.fit(bigram_Xs, coarse_train_y + coarse_dev_y)\n",
    "\n",
    "# BernoulliNB Results (Coarse)\n",
    "print(bernoulli_bigram_nb.score(bigram_test_Xs, coarse_test_y))\n",
    "print(bernoulli_bigram_nb.best_params_)\n",
    "\n",
    "# BernoulliNB Training (Fine)\n",
    "bernoulli_bigram_nb.fit(bigram_Xs, fine_train_y + fine_dev_y)\n",
    "\n",
    "# BernoulliNB Results (Fine)\n",
    "print(bernoulli_bigram_nb.score(bigram_test_Xs, fine_test_y))\n",
    "print(bernoulli_bigram_nb.best_params_)\n",
    "\n",
    "# MultinomialNB Training (Coarse)\n",
    "multinomial_bigram_nb = GridSearchCV(MultinomialNB(), nb_grid, cv=ps)\n",
    "multinomial_bigram_nb.fit(bigram_Xs, coarse_train_y + coarse_dev_y)\n",
    "\n",
    "# MultinomialNB Results (Coarse)\n",
    "print(multinomial_bigram_nb.score(bigram_test_Xs, coarse_test_y))\n",
    "print(multinomial_bigram_nb.best_params_)\n",
    "\n",
    "# MultinomialNB Training (Fine)\n",
    "multinomial_bigram_nb.fit(bigram_Xs, fine_train_y + fine_dev_y)\n",
    "\n",
    "# MultinomialNB Results(Fine)\n",
    "print(multinomial_bigram_nb.score(bigram_test_Xs, fine_test_y))\n",
    "print(multinomial_bigram_nb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "# Linear SVM Training (Coarse)\n",
    "svm = GridSearchCV(LinearSVC(), svm_grid, cv=ps)\n",
    "svm.fit(count_Xs, coarse_train_y + coarse_dev_y)\n",
    "\n",
    "# Linear SVM Results (Coarse)\n",
    "print(svm.score(count_test_Xs, coarse_test_y))\n",
    "print(svm.best_params_)\n",
    "\n",
    "# Linear SVM Training (Fine)\n",
    "svm.fit(count_Xs, fine_train_y + fine_dev_y)\n",
    "\n",
    "# Linear SVM Results (Fine)\n",
    "print(svm.score(count_test_Xs, fine_test_y))\n",
    "print(svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 9645\n",
      "Review 1000 of 9645\n",
      "Review 2000 of 9645\n",
      "Review 3000 of 9645\n",
      "Review 4000 of 9645\n",
      "Review 5000 of 9645\n",
      "Review 6000 of 9645\n",
      "Review 7000 of 9645\n",
      "Review 8000 of 9645\n",
      "Review 9000 of 9645\n",
      "Review 0 of 2210\n",
      "['hmm']\n",
      "Review 1000 of 2210\n",
      "Review 2000 of 2210\n",
      "['brimful']\n",
      "0.6149321266968326\n",
      "0.31266968325791855\n"
     ]
    }
   ],
   "source": [
    "# Word Vector Averaging\n",
    "\n",
    "def review_wordlist(review, remove_stopwords=False):\n",
    "    review_text = review\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return(words)\n",
    "\n",
    "num_features = 100  # Word vector dimensionality\n",
    "min_word_count = 5 # Minimum word count\n",
    "num_workers = 2     # Number of parallel threads\n",
    "context = 5        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "combined_training_sentences = list(train_sentences + dev_sentences)\n",
    "tokenized_sentences = []\n",
    "for sentence in combined_training_sentences:\n",
    "    tokenized_sentences.append(review_wordlist(sentence, remove_stopwords=False))\n",
    "\n",
    "model = word2vec.Word2Vec(tokenized_sentences, min_count=1)\n",
    "\n",
    "model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=10)\n",
    "\n",
    "# Function to average all word vectors in a review\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    if nwords == 0:\n",
    "        print(words)\n",
    "        return featureVec\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "# Function for calculating average word vectors for all reviews\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs\n",
    "    \n",
    "trainDataVecs = getAvgFeatureVecs(tokenized_sentences, model, num_features)\n",
    "\n",
    "filtered_test_reviews = []\n",
    "for review in test_sentences:\n",
    "    filtered_test_reviews.append(review_wordlist(review, remove_stopwords=False))\n",
    "    \n",
    "testDataVecs = getAvgFeatureVecs(filtered_test_reviews, model, num_features)\n",
    "\n",
    "clf = LinearSVC(max_iter=20000)\n",
    "clf.fit(trainDataVecs, coarse_train_y + coarse_dev_y)\n",
    "\n",
    "print(clf.score(testDataVecs, coarse_test_y))\n",
    "\n",
    "clf.fit(trainDataVecs, fine_train_y + fine_dev_y)\n",
    "\n",
    "print(clf.score(testDataVecs, fine_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Vector RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Tensor Neural Networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
